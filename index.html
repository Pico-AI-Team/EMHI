<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs</title>
    <link rel="icon" href="assets/images/VirtualRealityIcon.png">
    <link rel="stylesheet" href="css/extraAdd.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma-rtl.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
  </head>
  <body class="bg-light">
    <section class="section">
      <div class="container is-widescreen">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs</h1>
            
            <div class="is-size-5 publication-authors block">
              <span class="author-block">
                <a>Zhen Fan</a> ·
              </span>
              <span class="author-block">
                <a>Peng Dai</a> ·
              </span>
              <span class="author-block">
                <a>Zhuo Su</a> ·
              </span>
              <span class="author-block">
                <a">Xu Gao</a> ·
              </span>
              <span class="author-block">
                <a">Zheng Lv</a> ·
              </span>
              <span class="author-block">
                <a">JiaRui Zhang</a> ·
              </span>
              <span class="author-block">
                <a">Tianyuan Du</a> ·
              </span>
              <span class="author-block">
                <a">Guidong Wang</a> ·
              </span>
              <span class="author-block">
                <a>Yang Zhang</a> ·
              </span>
            </div>
  
            <div class="is-size-5 publication-authors block">
              <span class="company-block"><a href="https://www.picoxr.com/cn" target="_blank">Pico</a></span>
            </div>
            <div class="is-size-5 publication-info block">
              <span class="author-block">AAAI 2025 Accepted</span>
            </div>
          </div>
        </div>

        <div class="columns is-centered">
          <div class="column has-text-centered is-one-fifth">
            <div class="field">
              <div class="control">
                <div style="padding-left: 37%;"><figure class="image is-64x64"><img src="assets/images/paper.svg" alt="Download Paper"></figure></div>
                <a class="button is-primary" href="https://arxiv.org/abs/2408.17168" role="button" target="_blank">Paper</a>
              </div>
            </div>
         </div>
         <div class="column has-text-centered is-one-fifth">
          <div class="field">
            <div class="control">
              <div style="padding-left: 37%;"><figure class="image is-64x64"><img src="assets/images/code.svg" alt="Download Code"></figure></div>
              <a class="button is-success" href="https://github.com/Pico-AI-Team/EMHI" role="button" target="_blank">Code</a>
            </div>
          </div>
          </div>
          <div class="column has-text-centered is-one-fifth">
            <div class="field">
              <div class="control">
                <div style="padding-left: 37%;padding-bottom:2px;"><figure class="image is-64x64"><img src="assets/images/dataset.png" alt="Download Dataset"></figure></div>
                <a class="button is-primary" href="https://drive.google.com/file/d/1xEj3J0vJilx-jPCPbbsX6a6IF9LQ5URu/view?usp=drive_link" role="button" target="_blank">Dataset</a>
              </div>
            </div>
         </div>
          
        </div>
      </div>
    </section>

    <section class="section" style="padding-top: 0; padding-bottom: 0;">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered is-three-fifths">
            <!-- <div class="content is-large"><strong>Overview of Our Framework.</strong></div> -->
            <figure class="image"><img src="assets/images/overview.png" alt="Overview"></figure>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Egocentric human pose estimation (HPE) using wearable sensors is essential for VR/AR applications. 
                Most methods rely solely on either egocentric-view images or sparse Inertial Measurement Unit (IMU) signals, 
                leading to inaccuracies due to self-occlusion in images or the sparseness and drift of inertial sensors. 
                Most importantly, the lack of real-world datasets containing both modalities is a major obstacle to progress in this field. 
                To overcome the barrier, we propose EMHI, a multimodal Egocentric human Motion dataset with Head-Mounted Display (HMD) and body-worn IMUs, 
                with all data collected under the real VR product suite. 
                Specifically, EMHI provides synchronized stereo images from downward-sloping cameras on the headset and IMU data from body-worn sensors, 
                along with pose annotations in SMPL format. This dataset consists of 885 sequences captured by 58 subjects performing 39 actions,
                totaling about 28.5 hours of recording. We evaluate the annotations by comparing them with optical marker-based SMPL fitting results. 
                To substantiate the reliability of our dataset, we introduce MEPoser, a new baseline method for multimodal egocentric HPE, 
                which employs a multimodal fusion encoder, temporal feature encoder, and MLP-based regression heads. 
                The experiments on EMHI show that MEPoser outperforms existing single-modal methods and demonstrates the value of our dataset in solving the problem of egocentric HPE. 
                We believe the release of EMHI and the method could advance the research of egocentric HPE and expedite the practical implementation of this technology in VR/AR products.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Dataset Video -->
		<section class="hero is-small is-light">
			<div class="hero-head has-text-centered">
        <div class="content is-large "><p>Visualization of EMHI</p></div>
      </div>
			<div class="hero-body">
        <div ckass="container">
          <div id="carousel-Dataset-Demo" class="hero-carousel results-carousel">
            <div class="item">
              <video controls controlslist="nodownload" muted loop playsinline height="70%">
                <source src="assets/videos/cutting.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item">
              <video controls controlslist="nodownload" muted loop playsinline height="70%">
                <source src="assets/videos/sitting.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item">
              <video controls controlslist="nodownload" muted loop playsinline height="70%">
                <source src="assets/videos/下蹲.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item">
              <video controls controlslist="nodownload" muted loop playsinline height="70%">
                <source src="assets/videos/踢腿.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item">
              <video controls controlslist="nodownload" muted loop playsinline height="70%">
                <source src="assets/videos/太极.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item">
              <video controls controlslist="nodownload" muted loop playsinline height="70%">
                <source src="assets/videos/跪地.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
		</section>
		<!-- End OurDataSet Demo -->

    <!-- Dataset acquirment pipe -->
    <section class="section">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered is-three-fifths">
            <div class="content is-large">Data Collection Pipeline</div>
            <figure class="image"><img src="assets/images/collectionpipe.png" alt="Data Collection Pipeline"></figure>
          </div>
        </div>
      </div>
    </section>
    
    <!-- MEPoser Pipe -->
    <section class="section">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered is-three-fifths">
            <div class="content is-large">MEPoser Framework</div>
            <figure class="image"><img src="assets/images/meposer.png" alt="MEPoser FrameWork"></figure>
          </div>
        </div>
      </div>
    </section>

    <!-- MEPoser Video -->
		<section class="section">
			<div class="hero-head has-text-centered">
        <div class="content is-large "><p>Comparation Results</p></div>
      </div>
			<video controls="controls">
        <source src="assets/videos/meposer.mp4" type="type/mp4">
		</section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
          @article{fan2024emhi,
            title={EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs},
            author={Fan, Zhen and Dai, Peng and Su, Zhuo and Gao, Xu and Lv, Zheng and Zhang, Jiarui and Du, Tianyuan and Wang, Guidong and Zhang, Yang},
            journal={arXiv preprint arXiv:2408.17168},
            year={2024}
          }         
        </code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered">
        </div>
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>Copyright © EMHI Authors. All rights reserved.</p>
              <a href="https://www.flaticon.com/free-icons/vr-glasses" title="vr glasses icons" target="_blank">Vr glasses icons created by Freepik - Flaticon</a>
            </div>
          </div>
        </div>
      </div>
    </footer>
    
    <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
		<script>
			bulmaCarousel.attach('#carousel-3IMU-Demo', {
        initialSlide: 0,
				slidesToScroll: 1,
				slidesToShow: 3,
        navigation: true,
        navigationSwipe: true
			});
			bulmaCarousel.attach('#carousel-5IMU-Demo', {
        initialSlide: 0,
				slidesToScroll: 1,
				slidesToShow: 3,
        navigation: true,
        navigationSwipe: true
			});
			bulmaCarousel.attach('#carousel-6IMU-Demo', {
        initialSlide: 0,
				slidesToScroll: 1,
				slidesToShow: 3,
        navigation: true,
        navigationSwipe: true
			});
			bulmaCarousel.attach('#carousel-Dataset-Demo', {
        initialSlide: 0,
				slidesToScroll: 1,
				slidesToShow: 3,
        navigation: true,
        navigationSwipe: true
			});
		</script>
  </body>
</html>
